#!/usr/bin/env node

'use strict';

// To require local modules from root.
global.ROOT = __dirname + '/../dist/';

const colors = require('colors');
const fs = require('fs');
const path = require('path');
const vm = require('vm');

const GRAMMAR_MODE = require(ROOT + 'grammar/grammar-mode').MODES;
const Grammar = require(ROOT + 'grammar/grammar').default;

const options = require('nomnom')
  .script('syntax')
  .options({
    mode: {
      abbr: 'm',
      help: `Parser mode: ${getModesList()}`,
      transform: normalizeMode,
    },
    grammar: {
      abbr: 'g',
      help: 'File containing LL or LR grammar',
      metavar: 'FILE'
    },
    lex: {
      abbr: 'l',
      help: 'File containing lexical grammar',
      required: false,
      metavar: 'FILE'
    },
    table: {
      abbr: 't',
      help: 'Generate and output parsing table',
      flag: true,
    },
    collection: {
      abbr: 'c',
      help: 'Generate and output canonical collection of LR items',
      flag: true,
    },
    sets: {
      abbr: 's',
      help: 'Generate and output parsing sets (all/first/follow/predict)',
    },
    parse: {
      abbr: 'p',
      help: 'Parse a string and checks for acceptance',
      type: 'string',
    },
    file: {
      abbr: 'f',
      help: 'File to be parsed',
      type: 'string',
      metavar: 'FILE'
    },
    output: {
      abbr: 'o',
      help: 'Output file for a generated parser module',
      type: 'string',
      metavar: 'FILE'
    },
    'custom-tokenizer': {
      abbr: 'k',
      help: 'Path to a file with custom tokenizer class',
      type: 'string',
      metavar: 'FILE'
    },
    'tokenizer-only': {
      help: 'Whether to generate only standalone tokenizer output file',
      flag: true,
    },
    'tokenize': {
      help: 'Show list of tokens',
      flag: true,
    },
    'ignore-whitespaces': {
      abbr: 'w',
      help: 'Adds a Lex rule to ignore whitespaces',
      flag: true,
    },
    'resolve-conflicts': {
      abbr: 'r',
      help: 'Whether to auto-resolve conflicts with default action',
      flag: true,
    },
    'generate-inline-parser': {
      help: 'Whether to generate a parser module for parsing a passed string',
      flag: true,
    },
    'loc': {
       help: 'Capture token locations (offsets, line and column numbers)',
       flag: true,
    },
    'case-insensitive': {
       help: 'Sets case-insensitive mode to lexical grammar',
       abbr: 'i',
       flag: true,
    },
    namespace: {
      help: 'Append a wrapping namespace to generated code',
      type: 'string',
    },
  })
  .parse();

/**
 * Default generator options.
 */
const generatorOptions = {
  customTokenizer: options['custom-tokenizer'],
  resolveConflicts: options['resolve-conflicts'],
  namespace: options['namespace'],
};

/**
 * Set of parsers.
 */
const parsers = {
  LR0(options) {
    return this._genericLR(GRAMMAR_MODE.LR0, options);
  },

  SLR1(options) {
    return this._genericLR(GRAMMAR_MODE.SLR1, options);
  },

  CLR1(options) {
    return this._genericLR(GRAMMAR_MODE.CLR1, options);
  },

  LALR1(options) {
    return this._genericLR(GRAMMAR_MODE.LALR1, options);
  },

  _genericLR(mode, options) {
    const grammar = getGrammar(options.grammar, mode);

    console.info(`\nParsing mode: ${grammar.getMode()}.`);

    // Canonical collection or LR items.
    if (options.collection) {
      const CanonicalCollection =
        require(ROOT + 'lr/canonical-collection').default;

      new CanonicalCollection({grammar})
        .print();
    }

    // LR parsing table.
    if (options.table) {
      const CanonicalCollection =
        require(ROOT + 'lr/canonical-collection').default;

      const LRParsingTable = require(ROOT + 'lr/lr-parsing-table').default;

      new LRParsingTable({
        grammar,
        canonicalCollection: new CanonicalCollection({grammar}),
        resolveConflicts: options['resolve-conflicts'],
      }).print();
    }

    // Parse a string.
    if (provided('parse')) {
      parse(options.parse, grammar);
    }

    // Parse a file.
    if (provided('file')) {
      parse(fs.readFileSync(options.file, 'utf-8'), grammar);
    }

    // Output information about tokens.
    if (options['tokenize']) {
      tokenizeFromOptions(options, grammar.getLexGrammar());
    }

    // Generate parser module.
    if (options.output) {
      const outputFile = options.output;
      const language = path.extname(outputFile).slice(1);

      // Generator is language agnostic.
      const GENERATORS = {
        // Default.
        'js' : require(ROOT + 'lr/lr-parser-generator-default.js').default,

        // Plugins.
        'example' : require(ROOT + 'plugins/example/lr/lr-parser-generator-example.js').default,

        'py' : require(ROOT + 'plugins/python/lr/lr-parser-generator-py.js').default,
        'php': require(ROOT + 'plugins/php/lr/lr-parser-generator-php.js').default,
        'rb' : require(ROOT + 'plugins/ruby/lr/lr-parser-generator-ruby.js').default,
        'cs' : require(ROOT + 'plugins/csharp/lr/lr-parser-generator-csharp.js').default,
      };

      const LRParserGenerator = GENERATORS[language] || GENERATORS.js;

      new LRParserGenerator({
        grammar,
        outputFile,
        options: generatorOptions,
      }).generate();

      showGeneratedSuccessMessage(options.output);
    }
  },

  LL1(options) {
    const grammar = getGrammar(options.grammar, GRAMMAR_MODE.LL1);

    console.info(`\nParsing mode: ${grammar.getMode()}.`);

    // LL parsing table.
    if (options.table) {
      const LLParsingTable = require(ROOT + 'll/ll-parsing-table').default;

      new LLParsingTable({
        grammar,
      }).print();
    }

    // Parse a string.
    if (provided('parse')) {
      parse(options.parse, grammar);
    }

    // Parse a file.
    if (provided('file')) {
      parse(fs.readFileSync(options.file, 'utf-8'), grammar);
    }

    // Output information about tokens.
    if (options['tokenize']) {
      tokenizeFromOptions(options, grammar.getLexGrammar());
    }

    // Generate parser module.
    if (options.output) {
      const outputFile = options.output;
      const language = path.extname(outputFile).slice(1);

      // Generator is language agnostic.
      const GENERATORS = {
        // Default.
        'js': require(ROOT + 'll/ll-parser-generator-default.js').default,

        // Plugins.
        'example': require(ROOT + 'plugins/example/ll/ll-parser-generator-example.js').default,

        'py': require(ROOT + 'plugins/python/ll/ll-parser-generator-py.js').default,
        'php': require(ROOT + 'plugins/php/ll/ll-parser-generator-php.js').default,
        'rb' : require(ROOT + 'plugins/ruby/ll/ll-parser-generator-ruby.js').default,
      };

      const LLParserGenerator = GENERATORS[language];

      new LLParserGenerator({
        grammar,
        outputFile,
        options: generatorOptions,
      }).generate();

      showGeneratedSuccessMessage(options.output);
    }
  },
};

function showGeneratedSuccessMessage(filePath) {
  console.info(
    `${colors.green('\n\u2713 Successfully generated:')}`,
    filePath, '\n'
  );
}

function parse(string, grammar) {
  console.info(`\n${colors.bold('Parsing:')}\n\n${string}\n`);

  try {
    const parsed = grammar.getMode().isLR()
      ? lrParse(string, grammar)
      : llParse(string, grammar);

    if (parsed.status === 'accept') {
      console.info(`${colors.green('\u2713 Accepted')}\n`);
    }

    if (parsed.hasOwnProperty('value')) {
      console.info(
        colors.bold('Parsed value:'), '\n\n' +
        formatParsedOutput(parsed.value), '\n'
      );
    }

  } catch (e) {
    console.info(`${colors.red(e.stack)}\n`);
    process.exit(1);
  }
}

function lrParse(string, grammar) {
  const LRParser = require(ROOT + 'lr/lr-parser').default;

  if (options['generate-inline-parser']) {
    return LRParser.fromParserGenerator({grammar}).parse(string);
  }

  return new LRParser({grammar}).parse(string);
}

function llParse(string, grammar) {
  const LLParser = require(ROOT + 'll/ll-parser').default;

  if (options['generate-inline-parser']) {
    return LLParser.fromParserGenerator({grammar}).parse(string);
  }

  return new LLParser({grammar}).parse(string);
}

function formatParsedOutput(output) {
  // Object constructor is used from another realm, so no direct
  // constructor check, neither `instanceof` would work. Check
  // `name` property.
  if (
    Array.isArray(output) ||
    (output && output.constructor && output.constructor.name === 'Object')
  ) {
    return JSON.stringify(output, null, 2);
  }
  return output;
}

function getGrammar(grammarFile, mode) {
  if (!grammarFile) {
    return null;
  }

  const grammarData = Grammar.dataFromGrammarFile(grammarFile, 'bnf');

  // If explicit lexical grammar file was passed, use it.
  const lexGrammarData = getLexGrammarData(options);
  if (lexGrammarData) {
    grammarData.lex = Object.assign(
      {},
      grammarData.lex || {},
      lexGrammarData
    );
  }

  const grammarOptions = {
    /**
     * Parsing mode.
     */
    mode,

    /**
     * Whether to capture locations.
     */
    captureLocations: options.loc,
  };

  return Grammar.fromData(grammarData, grammarOptions);
}

function getLexGrammarData(options) {
  let data = null;

  // If explicit lexical grammar file was passed, use it.
  if (options.lex) {
    data = Grammar.dataFromGrammarFile(options.lex, 'lex');
  }

  if (options['ignore-whitespaces'] && !data) {
    data = {
      rules: [
        ['\\s+', /* skip whitespace */''],
      ],
    };
  }

  if (options['case-insensitive']) {
    if (!data) {
      data = {};
    }

    if (!data.options) {
      data.options = {};
    }

    data.options['case-insensitive'] = true;
  }

  return data;
}

function normalizeMode(mode) {
  return mode.toUpperCase();
}

function getModesList() {
  return Object.keys(GRAMMAR_MODE).join(', ');
}

function extractMode(options) {
  let mode = options.mode;

  // If no explicit mode is passed, try
  // infer it from the grammar file extension.
  if (!mode && options.grammar) {
    mode = path.extname(options.grammar).slice(1);
  }

  if (!mode) {
    error(`\nError: "mode" option is required for parsing\n`);
    return null;
  }

  mode = normalizeMode(mode);

  if (!GRAMMAR_MODE.hasOwnProperty(mode)) {
    error(`\nError: "${mode}" is not a valid parsing mode. ` +
      `Valid modes are: ${getModesList()}.\n`
    );
    return null;
  }

  if (!parsers.hasOwnProperty(mode)) {
    let availableModes = Object.keys(parsers)
      .filter(mode => !mode.startsWith('_'))
      .join(', ');

    error(
      `\nError: "${mode}" is not implemented yet. ` +
      `Available parsers are: ${availableModes}.\n`
    );
    return null;
  }

  return (options.mode = mode);
}

function handleSets() {
  const SetsGenerator = require(ROOT + 'sets-generator').default;

  let sets = options.sets;
  let sg = new SetsGenerator({
    grammar: getGrammar(options.grammar, options.mode),
  });

  if (sets.indexOf('first') !== -1 || sets === 'all') {
    sg.printSet(sg.getFirstSets());
  }

  if (sets.indexOf('follow') !== -1 || sets === 'all') {
    sg.printSet(sg.getFollowSets());
  }

  if (sets.indexOf('predict') !== -1 || sets === 'all') {
    sg.printSet(sg.getPredictSets());
  }
}

function error(message) {
  console.error(colors.red(message));
  console.log('Run --help for details.\n');
  process.exit(1);
}

function isTokenizerOnly(options) {
  return options['tokenizer-only'] ||
    (options.lex && !options.grammar);
}

function handleStandaloneTokenizer() {
  const LexGrammar = require(ROOT + 'grammar/lex-grammar').default;

  let lexGrammarData = getLexGrammarData(options);
  let lexGrammar;

  if (lexGrammarData) {
    lexGrammar = new LexGrammar(lexGrammarData);
  } else {
    // Try infer from --grammar.
    lexGrammar = getGrammar(options.grammar, options.mode).getLexGrammar();
  }

  if (!provided('tokenize') && !provided('output')) {
    error('\nError: for tokenization pass either --tokenize or --output.\n');
  }

  if (options['tokenize']) {
    tokenizeFromOptions(options, lexGrammar);
  }
}

function provided(option) {
  return options.hasOwnProperty(option);
}

function tokenizeFromOptions(options, lexGrammar) {
  if (!provided('parse') && !provided('file')) {
    error('\nError: tokenization requires -p or -f parameter\n');
    return;
  }

  // Tokenize a string.
  if (provided('parse')) {
    tokenize(options.parse, lexGrammar);
  }

  // Tokinize a file.
  if (provided('file')) {
    tokenize(fs.readFileSync(options.file, 'utf-8'), lexGrammar);
  }
}

function tokenize(string, lexGrammar) {
  // Inline tokenization supported only for JS.
  const Tokenizer = require(ROOT + 'tokenizer').default;

  const tokens = (new Tokenizer({string, lexGrammar})).getTokens();

  // Don't show last EOF token.
  tokens.pop();

  console.info(
    colors.bold('\nList of tokens:'), '\n\n',
    formatParsedOutput(tokens), '\n'
  );
}

function main() {
  if (!options.grammar && !options.lex) {
    error(`\nError: expected at least --grammar or --lex parameters.\n`);
    return;
  }

  // Generating a standalone tokenizer, either from direct --lex
  // parameter, or from the `lex` part of the --grammar parameter.
  if (isTokenizerOnly(options)) {
    return handleStandaloneTokenizer();
  }

  // Sets.
  if (options.sets) {
    handleSets();
  }

  parsers[extractMode(options)](options);
}

if (require.main === module) {
    main();
}
